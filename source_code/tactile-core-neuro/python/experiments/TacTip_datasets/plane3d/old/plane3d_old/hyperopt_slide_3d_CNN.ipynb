{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import datetime\n",
    "import pickle\n",
    "import json\n",
    "import copy\n",
    "from functools import partial\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from hyperopt import tpe, hp, fmin, Trials, STATUS_OK, STATUS_FAIL\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras import layers, models, optimizers, regularizers, callbacks\n",
    "from keras import backend as K\n",
    "from keras_preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperopt helper function\n",
    "@scope.define\n",
    "def exp2(x):\n",
    "    return 2 ** x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decorator for multi-output layer generators\n",
    "class MultiOutputGenerator:\n",
    "    def __init__(self, gen):\n",
    "        self._gen = gen\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        ret = self._gen.next()\n",
    "        x, y = ret\n",
    "        y = [y_col for y_col in y.T] if y.ndim > 1 else [y]\n",
    "        return x, y\n",
    "    \n",
    "    @property\n",
    "    def n(self):\n",
    "        return self._gen.n\n",
    "    \n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self._gen.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure Tensorflow GPU options\n",
    "config = tf.ConfigProto(\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "    # device_count = {'GPU': 1}\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up data dirs and files\n",
    "#data_root_dir = os.environ[\"DATAPATH\"]\n",
    "data_root_dir = r\"D:\\Users\\nl13426\\Documents\\Data temp\"\n",
    "\n",
    "train_data_dir = os.path.join(data_root_dir, r\"TacTip-datasets\\plane3d\\collectSlideRand3dNframes05071937\")\n",
    "train_image_dir = os.path.join(train_data_dir, \"images\")\n",
    "train_df_file = os.path.join(train_data_dir, \"targets_image.csv\")\n",
    "train_df = pd.read_csv(train_df_file)\n",
    "\n",
    "valid_data_dir = os.path.join(data_root_dir, r\"TacTip-datasets\\plane3d\\collectSlideRand3dNframes05071938\")\n",
    "valid_image_dir = os.path.join(valid_data_dir, \"images\")\n",
    "valid_df_file = os.path.join(valid_data_dir, \"targets_image.csv\")\n",
    "valid_df = pd.read_csv(valid_df_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for pre-processing images\n",
    "def process_image(image, crop=None, resize=None, threshold=None): \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    if crop is not None:\n",
    "        x0, y0, x1, y1 = crop\n",
    "        image = image[y0:y1, x0:x1]\n",
    "    if threshold is not None:\n",
    "#        ret, image = cv2.threshold(image, int(threshold * 255), 255, cv2.THRESH_BINARY)\n",
    "        image = image.astype('uint8')\n",
    "        image = cv2.medianBlur(image, 5)\n",
    "        width, offset = threshold\n",
    "        image = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \\\n",
    "                                      cv2.THRESH_BINARY, width, offset)\n",
    "    if resize is not None:\n",
    "        image = cv2.resize(image, resize, interpolation=cv2.INTER_AREA)\n",
    "    image = image[..., np.newaxis]  \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop = (160, 70, 490, 400)\n",
    "resize = (128, 128)\n",
    "threshold = (11, -5)\n",
    "\n",
    "# pre-process training images\n",
    "pp_train_image_dir = os.path.join(train_image_dir, 'pp_images')\n",
    "if not os.path.isdir(pp_train_image_dir):\n",
    "    os.makedirs(pp_train_image_dir)\n",
    "    image_files = (f for f in os.listdir(train_image_dir) if f.endswith('.jpg'))\n",
    "    for image_file in image_files:\n",
    "        image = cv2.imread(os.path.join(train_image_dir, image_file))\n",
    "        image = process_image(image, crop=crop, resize=resize, threshold=threshold)\n",
    "        cv2.imwrite(os.path.join(pp_train_image_dir, image_file), image);\n",
    "        \n",
    "# pre-process validation images\n",
    "pp_valid_image_dir = os.path.join(valid_image_dir, 'pp_images')\n",
    "if not os.path.isdir(pp_valid_image_dir):\n",
    "    os.makedirs(pp_valid_image_dir)\n",
    "    image_files = (f for f in os.listdir(valid_image_dir) if f.endswith('.jpg'))\n",
    "    for image_file in image_files:\n",
    "        image = cv2.imread(os.path.join(valid_image_dir, image_file))\n",
    "        image = process_image(image, crop=crop, resize=resize, threshold=threshold)\n",
    "        cv2.imwrite(os.path.join(pp_valid_image_dir, image_file), image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build CNN model\n",
    "def build_model(num_conv_layers,\n",
    "                num_conv_filters,\n",
    "                num_dense_layers,\n",
    "                num_dense_units,\n",
    "                hidden_activation,\n",
    "                batch_norm,\n",
    "                dropout,\n",
    "                l1_norm,\n",
    "                l2_norm,\n",
    "                batch_size,\n",
    "                ):\n",
    "    \n",
    "    input_dims = (128, 128, 1)\n",
    "    output_dim = 3\n",
    "    num_hidden_layers = num_conv_layers + num_dense_layers\n",
    "    num_layers = num_hidden_layers + 1\n",
    "\n",
    "    # convolutional layers\n",
    "    inp = keras.Input(shape=input_dims, name='input')\n",
    "    x = inp\n",
    "    for i in range(num_conv_layers):\n",
    "        x = layers.Conv2D(filters=num_conv_filters, kernel_size=3)(x)\n",
    "        if batch_norm:\n",
    "            x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation(hidden_activation)(x)\n",
    "        x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    # dense layers\n",
    "    x = layers.Flatten()(x)\n",
    "    for i in range(num_conv_layers, num_hidden_layers):\n",
    "        if dropout > 0:\n",
    "            x = layers.Dropout(dropout)(x)\n",
    "        x = layers.Dense(units=num_dense_units,\n",
    "            kernel_regularizer=regularizers.l1_l2(l1=l1_norm, l2=l2_norm))(x)         \n",
    "        x = layers.Activation(hidden_activation)(x)            \n",
    "\n",
    "    # Output layers\n",
    "    if dropout > 0:\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "    out = [layers.Dense(units=1,\n",
    "                name=('output_' + str(i + 1)),\n",
    "                kernel_regularizer=regularizers.l1_l2(l1=l1_norm, l2=l2_norm))(x)\n",
    "                for i in range(output_dim)]\n",
    "\n",
    "    model = models.Model(inp, out)\n",
    "    model.compile(optimizer=optimizers.Adam(lr=1e-4, decay=1e-6),\n",
    "                                            loss=['mse',] * output_dim,\n",
    "                                            metrics=['mae']) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build hyperopt objective function\n",
    "def build_objective_func():\n",
    "    trial = 1\n",
    "    \n",
    "    def objective_func(args):\n",
    "        nonlocal trial\n",
    "        global model_dir\n",
    "\n",
    "        print(\"Trial: {}\".format(trial))\n",
    "        \n",
    "        # unpack params\n",
    "        num_conv_layers = args['num_conv_layers']\n",
    "        num_conv_filters = args['num_conv_filters']\n",
    "        num_dense_layers = args['num_dense_layers']\n",
    "        num_dense_units = args['num_dense_units']\n",
    "        hidden_activation = args['hidden_activation']\n",
    "        batch_norm = args['batch_norm']\n",
    "        dropout = args['dropout']\n",
    "        l1_norm = args['l1_norm']\n",
    "        l2_norm = args['l2_norm']\n",
    "        batch_size = args['batch_size']\n",
    "\n",
    "        # print params\n",
    "        print(\"num_conv_layers: {}\".format(num_conv_layers))\n",
    "        print(\"num_conv_filters: {}\".format(num_conv_filters))\n",
    "        print(\"num_dense_layers: {}\".format(num_dense_layers))\n",
    "        print(\"num_dense_units: {}\".format(num_dense_units))\n",
    "        print(\"hidden_activation: {}\".format(hidden_activation))\n",
    "        print(\"batch_norm: {}\".format(batch_norm))\n",
    "        print(\"dropout: {:0.2}\".format(dropout))\n",
    "        print(\"l1_norm: {:0.2}\".format(l1_norm))\n",
    "        print(\"l2_norm: {:0.2}\".format(l2_norm))\n",
    "        print(\"batch_size: {}\".format(batch_size))\n",
    "\n",
    "        # build CNN model\n",
    "        K.clear_session()\n",
    "        model = build_model(num_conv_layers,\n",
    "                            num_conv_filters,\n",
    "                            num_dense_layers,\n",
    "                            num_dense_units,\n",
    "                            hidden_activation,\n",
    "                            batch_norm,\n",
    "                            dropout,\n",
    "                            l1_norm,\n",
    "                            l2_norm,\n",
    "                            batch_size,\n",
    "                           )\n",
    "        num_params = model.count_params()\n",
    "        print(\"Model params: {:,}\".format(num_params))\n",
    "\n",
    "        # other training parameters\n",
    "        x_col = 'sensor_image'\n",
    "        y_col = ['pose_3', 'pose_4', 'pose_5']\n",
    "        class_mode='other'\n",
    "        target_size = (128, 128)\n",
    "        color_mode = 'grayscale'\n",
    "        patience = 10\n",
    "        epochs = 300\n",
    "\n",
    "        # set up data generators\n",
    "#        train_datagen = ImageDataGenerator(rescale=1./255., validation_split=0.2)\n",
    "        train_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "        valid_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "        \n",
    "        train_generator = train_datagen.flow_from_dataframe(\n",
    "            dataframe=train_df,\n",
    "            directory=pp_train_image_dir,\n",
    "            x_col=x_col,\n",
    "            y_col=y_col,\n",
    "#            subset='training',\n",
    "            batch_size=batch_size,\n",
    "            seed=42,\n",
    "            shuffle=True,\n",
    "            class_mode=class_mode,\n",
    "            target_size=target_size,\n",
    "            color_mode=color_mode,\n",
    "        )\n",
    "\n",
    "        valid_generator = valid_datagen.flow_from_dataframe(\n",
    "            dataframe=valid_df,\n",
    "            directory=pp_valid_image_dir,\n",
    "            x_col=x_col,\n",
    "            y_col=y_col,\n",
    "#            subset='validation',\n",
    "            batch_size=batch_size,\n",
    "            seed=42,\n",
    "            shuffle=True,\n",
    "            class_mode=class_mode,\n",
    "            target_size=target_size,\n",
    "            color_mode=color_mode,\n",
    "        )\n",
    "        \n",
    "        train_generator = MultiOutputGenerator(train_generator)\n",
    "        valid_generator = MultiOutputGenerator(valid_generator)    \n",
    "        step_size_train = train_generator.n // train_generator.batch_size\n",
    "        step_size_valid = valid_generator.n // valid_generator.batch_size\n",
    "\n",
    "        # fit model\n",
    "        print(\"Training model ...\")\n",
    "        model_file = \"best_model_\" + str(trial) + \".h5\"\n",
    "        train_callbacks = [\n",
    "            callbacks.EarlyStopping(monitor='val_loss', patience=patience, verbose=1),\n",
    "            callbacks.ModelCheckpoint(\n",
    "                filepath=os.path.join(model_dir, model_file),\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True,\n",
    "            )\n",
    "        ]\n",
    "        try:\n",
    "            history = model.fit_generator(generator=train_generator,\n",
    "                                          steps_per_epoch=step_size_train,\n",
    "                                          validation_data=valid_generator,\n",
    "                                          validation_steps=step_size_valid,\n",
    "                                          epochs=epochs,\n",
    "                                          verbose=0,\n",
    "                                          callbacks=train_callbacks)\n",
    "        except tf.errors.ResourceExhaustedError:\n",
    "            loss = None\n",
    "            status = STATUS_FAIL\n",
    "            stopped_epoch = None\n",
    "            history = None\n",
    "            print(\"Aborted trial: Resource exhausted error\\n\")\n",
    "        else:\n",
    "            loss = np.min(history.history['val_loss'])\n",
    "            status = STATUS_OK\n",
    "            stopped_epoch = len(history.history['val_loss'])\n",
    "            history = history.history\n",
    "            print(\"Loss: {:0.2}\\n\".format(loss)) \n",
    "\n",
    "        results = {'loss': loss, 'status': status, 'stopped_epoch': stopped_epoch,\n",
    "                   'history': history, 'num_params': num_params, 'trial': trial,}\n",
    "        trial += 1\n",
    "        return results  \n",
    "    \n",
    "    return objective_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify hyperopt search space\n",
    "# num_conv_layers = scope.int(hp.quniform(label='num_conv_layers', low=1, high=5, q=1))\n",
    "space = {\n",
    "#    'num_conv_layers': num_conv_layers,\n",
    "#    'num_conv_filters': scope.int(scope.exp2(scope.switch(num_conv_layers - 1,\n",
    "#        hp.quniform(label='num_conv_filters_1', low=1, high=8, q=1), \n",
    "#        hp.quniform(label='num_conv_filters_2', low=1, high=7, q=1),\n",
    "#        hp.quniform(label='num_conv_filters_3', low=1, high=6, q=1),\n",
    "#        hp.quniform(label='num_conv_filters_4', low=1, high=5, q=1),\n",
    "#        hp.quniform(label='num_conv_filters_5', low=1, high=4, q=1),\n",
    "#    ))),\n",
    "    'num_conv_layers': scope.int(hp.quniform(label='num_conv_layers', low=1, high=5, q=1)),\n",
    "    'num_conv_filters': scope.int(scope.exp2(hp.quniform(label='num_conv_filters', low=1, high=9, q=1))),\n",
    "    'num_dense_layers': scope.int(hp.quniform(label='num_dense_layers', low=1, high=5, q=1)),\n",
    "    'num_dense_units': scope.int(scope.exp2(hp.quniform(label='num_dense_units', low=1, high=9, q=1))),\n",
    "    'hidden_activation':  hp.choice(label='hidden_activation', options=('relu', 'elu')),\n",
    "    'batch_norm':  hp.choice(label='batch_norm', options=(False, True)),\n",
    "    'dropout': hp.uniform(label='dropout', low=0, high=0.5),\n",
    "    'l1_norm': hp.loguniform(label='l1_norm', low=-4*math.log(10), high=-math.log(10)),\n",
    "    'l2_norm': hp.loguniform(label='l2_norm', low=-4*math.log(10), high=-math.log(10)),\n",
    "    'batch_size': scope.int(scope.exp2(hp.quniform(label='batch_size', low=5, high=8, q=1))),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for formatting returned hyperparams\n",
    "def format_params(params):\n",
    "    params_conv = copy.deepcopy(params)\n",
    "    params_conv['batch_norm'] = True if params['batch_norm'] == 1 else False\n",
    "    params_conv['batch_size'] = int(2 ** params['batch_size'])\n",
    "    params_conv['hidden_activation'] = ('relu', 'elu')[params['hidden_activation']]\n",
    "#    num_conv_layers = int(params['num_conv_layers'])\n",
    "#    params_conv['num_conv_layers'] = num_conv_layers\n",
    "#    del_keys = [k for k in params_conv.keys() if k.lower().startswith('num_conv_filters_')]\n",
    "#    for k in del_keys:\n",
    "#        del params_conv[k]\n",
    "#    params_conv['num_conv_filters'] = int(2 ** params['num_conv_filters_' + str(num_conv_layers)])\n",
    "    params_conv['num_conv_layers'] = int(params['num_conv_layers'])\n",
    "    params_conv['num_conv_filters'] = int(2 ** params['num_conv_filters'])\n",
    "    params_conv['num_dense_layers'] = int(params['num_dense_layers'])\n",
    "    params_conv['num_dense_units'] = int(2 ** params['num_dense_units'])\n",
    "    \n",
    "    return params_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial: 1                                                                                                               \n",
      "num_conv_layers: 4                                                                                                     \n",
      "num_conv_filters: 16                                                                                                   \n",
      "num_dense_layers: 1                                                                                                    \n",
      "num_dense_units: 4                                                                                                     \n",
      "hidden_activation: elu                                                                                                 \n",
      "batch_norm: True                                                                                                       \n",
      "dropout: 0.41                                                                                                          \n",
      "l1_norm: 0.00034                                                                                                       \n",
      "l2_norm: 0.00025                                                                                                       \n",
      "batch_size: 64                                                                                                         \n",
      "Model params: 9,699                                                                                                    \n",
      "Found 2000 validated image filenames.                                                                                  \n",
      "Found 2000 validated image filenames.                                                                                  \n",
      "Training model ...                                                                                                     \n",
      "  0%|                                                                            | 0/300 [00:01<?, ?it/s, best loss: ?]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1fb689b98c54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtrials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mobj_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_objective_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mbest_model_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_startup_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# save (formatted) best model params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nathan\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[0mshow_progressbar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m         )\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nathan\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar)\u001b[0m\n\u001b[0;32m    637\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m             show_progressbar=show_progressbar)\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nathan\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[0;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[0;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nathan\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nathan\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    225\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m                         \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nathan\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nathan\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 844\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-7f497a4a0706>\u001b[0m in \u001b[0;36mobjective_func\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    114\u001b[0m                                           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                                           \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m                                           callbacks=train_callbacks)\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mResourceExhaustedError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nathan\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nathan\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nathan\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    232\u001b[0m                             \u001b[0mval_enqueuer_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m                             workers=0)\n\u001b[0m\u001b[0;32m    235\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m                         \u001b[1;31m# No need for try/except because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nathan\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nathan\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m   1470\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1471\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m             verbose=verbose)\n\u001b[0m\u001b[0;32m   1473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nathan\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m    344\u001b[0m                                  \u001b[1;34m'or (x, y). Found: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                                  str(generator_output))\n\u001b[1;32m--> 346\u001b[1;33m             \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mouts_per_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nathan\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[1;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[0;32m   1254\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1256\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1257\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nathan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nathan\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\nathan\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set up model dir\n",
    "model_dir = os.path.join(train_data_dir, \"models\", \"opt_\" + datetime.datetime.now().strftime(\"%m%d%H%M\"))\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# perform optimization\n",
    "trials = Trials()\n",
    "obj_func = build_objective_func()\n",
    "best_model_params = fmin(obj_func, space, algo=partial(tpe.suggest, n_startup_jobs=50), max_evals=300, trials=trials)\n",
    "\n",
    "# save (formatted) best model params\n",
    "with open(os.path.join(model_dir, \"best_model_params.json\"), \"w\") as f:\n",
    "    json.dump(format_params(best_model_params), f)\n",
    "    \n",
    "# save trials history\n",
    "with open(os.path.join(model_dir, \"trials.pickle\"), \"wb\") as f:\n",
    "    pickle.dump(trials, f)\n",
    "\n",
    "# save results in CSV format\n",
    "trials_df = pd.DataFrame()\n",
    "for i, trial in enumerate(trials):\n",
    "    trial_params = {k: v[0] if len(v) > 0 else None for k, v in trial['misc']['vals'].items()}\n",
    "    trial_row = pd.DataFrame(format_params(trial_params), index=[i])\n",
    "    trial_row['tid'] = trial['tid']\n",
    "    trial_row['loss'] = trial['result']['loss']\n",
    "    trial_row['status'] = trial['result']['status']\n",
    "    trial_row['trial'] = trial['result']['trial']\n",
    "    trial_row['stopped_epoch'] = trial['result']['stopped_epoch']\n",
    "    trial_row['num_params'] = trial['result']['num_params']\n",
    "    trials_df = pd.concat([trials_df, trial_row])\n",
    "# trials_df['loss'] = trials.losses()\n",
    "trials_df.to_csv(os.path.join(model_dir, \"results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model trial: 1\n",
      "Best model loss: inf\n",
      "Best model params:\n",
      "{'batch_norm': False, 'batch_size': 32, 'dropout': 1.1475282350913857e-05, 'hidden_activation': 'relu', 'l1_norm': 0.00010034007490647624, 'l2_norm': 0.0006147269543364173, 'num_conv_filters': 512, 'num_conv_layers': 5, 'num_dense_layers': 1, 'num_dense_units': 16}\n"
     ]
    }
   ],
   "source": [
    "# print results\n",
    "losses = copy.deepcopy(trials.losses())\n",
    "losses = [l if l is not None else np.inf for l in losses]\n",
    "\n",
    "print(\"Best model trial: {}\".format(np.argmin(losses) + 1))\n",
    "print(\"Best model loss: {:0.4}\".format(np.min(losses)))\n",
    "print(\"Best model params:\")\n",
    "print(format_params(best_model_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\Users\\\\nl13426\\\\Documents\\\\Data temp\\\\TacTip-datasets\\\\plane3d\\\\collectSlideRand3dNframes05071937\\\\models\\\\opt_11032122\\\\trials.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-3b30124d84d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load and print trials history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"trials.pickle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtrials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\Users\\\\nl13426\\\\Documents\\\\Data temp\\\\TacTip-datasets\\\\plane3d\\\\collectSlideRand3dNframes05071937\\\\models\\\\opt_11032122\\\\trials.pickle'"
     ]
    }
   ],
   "source": [
    "# load and print trials history\n",
    "with open(os.path.join(model_dir, \"trials.pickle\"), \"rb\") as f:\n",
    "    trials = pickle.load(f)\n",
    "    \n",
    "for t in trials.trials:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
